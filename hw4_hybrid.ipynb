{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'ignore', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "import os\n",
    "import pickle as pkl\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "np.seterr(divide='ignore') # masks log(0) errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hybrid.hmm.multiple import FullGaussianHMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default DNN set-up should take ~40 seconds/epoch on a GPU (and ~350 secconds/epoch on a CPU).\n",
    "\n",
    "Performance (WER) on test set:   \n",
    "\n",
    "Baseline performance of the GMM-HMM model   \n",
    "24.55%\n",
    "\n",
    "Performance of the DNN-HMM model with normalized emission probabilities   \n",
    "20.45%\n",
    "\n",
    "Performance of the DNN-HMM model with unnormalized emission probabilities   \n",
    "18.18%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a multiple digit GMM-HMM model\n",
    "NOTE: You are not expected to run/tune this part as the trained FullGaussianHMM model file is provided. The provided model is designed to have 15 states for each digit and 3 additional states for start, pause, and end. Feel free to look through hybrid/hmm/multiple.py to see how we can string single-digit HMMs to obtain the one that can model multiple-digit sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Multiple Digit HMM: training two-digit sequences\n",
    "# \"\"\"\n",
    "# data_multiple_digit = np.load(\"hybrid/data/mfccs/mfccs_multiple.npz\", allow_pickle=True)\n",
    "# full_model = FullGaussianHMM(data_multiple_digit[\"Xtrain\"], \"hybrid/hmm/models/single_digit_model.pkl\")\n",
    "\n",
    "# n_iter = 15\n",
    "\n",
    "# print(\"Training HMM\")\n",
    "# for i in range(n_iter):\n",
    "#     print(\"starting iteration {}...\".format(i + 1))\n",
    "#     full_model.train(data_multiple_digit[\"Xtrain\"], data_multiple_digit[\"Ytrain\"])\n",
    "        \n",
    "# print(\"Testing HMM\")\n",
    "# test_wer = full_model.test(data_multiple_digit[\"Xtest\"], data_multiple_digit[\"Ytest\"])\n",
    "# print(\"{:.2f}% WER\".format(test_wer * 100.))\n",
    "\n",
    "# with open(\"hybrid/hmm/models/multiple_digit_model.pkl\", \"wb\") as f:\n",
    "#     pkl.dump(full_model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the optimal state sequences\n",
    "Save the optimal state label per framee using the trained GMM-HMM model. Complete the # TODO in force_align function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def force_align(X, Y, hmm_gmm_model):\n",
    "    \"\"\"\n",
    "    Force align using Viterbi to get the hidden state sequence for each (X, Y) pair.\n",
    "    ------\n",
    "    input:\n",
    "    X: list of 2d-arrays of shape (Tx, 13): list of single digit MFCC features\n",
    "    Y: digit sequence\n",
    "    hmm_gmm_model: load the trained model\n",
    "    ------\n",
    "    Returns a list of utterence-wise hidden state sequences\n",
    "    \"\"\"\n",
    "    digit_states_total, start_states = hmm_gmm_model.digit_states_total, hmm_gmm_model.start_states\n",
    "    ## 150 states, 15 for each \n",
    "    begin_sil_id, pause_id, end_sil_id = hmm_gmm_model.begin_sil, hmm_gmm_model.pause, hmm_gmm_model.end_sil\n",
    "    ## ids are 150,152,151 for each of them\n",
    "    A_estimate, pi_estimate = hmm_gmm_model.A, hmm_gmm_model.pi\n",
    "    state_seqs = []\n",
    "    for ii, (x, y) in enumerate(zip(X, Y)):\n",
    "\n",
    "        y = np.array([0 if yy == 'o' else int(yy) for yy in y], dtype=np.int32)\n",
    "        # TODO: edit A_estimate appropriately to enable decoding for the ground-truth labelss\n",
    "        \n",
    "        A_estimate[begin_sil_id, :digit_states_total] = 0.\n",
    "        A_estimate[pause_id, :digit_states_total] = 0.\n",
    "\n",
    "        A_estimate[begin_sil_id, start_states[y[0]]] = 1. - A_estimate[begin_sil_id, begin_sil_id]\n",
    "        A_estimate[pause_id, start_states[y[1]]] = 1. - A_estimate[pause_id, pause_id]\n",
    "   \n",
    "        log_pi = np.log(pi_estimate)\n",
    "        log_A = np.log(A_estimate)\n",
    "        log_B = hmm_gmm_model.get_emissions(x)\n",
    "\n",
    "        q, log_prob = hmm_gmm_model.viterbi(log_pi, log_A, log_B) \n",
    "        state_seqs.append(q)\n",
    "\n",
    "    return state_seqs\n",
    "\n",
    "data_multiple_digit = np.load(\"hybrid/data/mfccs/mfccs_multiple.npz\", allow_pickle=True)\n",
    "with open(\"hybrid/hmm/models/multiple_digit_model.pkl\", \"rb\") as f:\n",
    "    hmm_gmm_model = pkl.load(f)\n",
    "    \n",
    "state_seq_train = force_align(data_multiple_digit[\"Xtrain\"], data_multiple_digit[\"Ytrain\"], hmm_gmm_model)\n",
    "state_seq_dev = force_align(data_multiple_digit[\"Xdev\"], data_multiple_digit[\"Ydev\"], hmm_gmm_model)\n",
    "state_seq_test = force_align(data_multiple_digit[\"Xtest\"], data_multiple_digit[\"Ytest\"], hmm_gmm_model)\n",
    "seqDict = {'Ytrain': state_seq_train, 'Ydev': state_seq_dev, 'Ytest': state_seq_test, 'total_states': hmm_gmm_model.total}\n",
    "np.savez_compressed('hybrid/data/state_seq/state_seq.npz', **seqDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a DNN frame classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hybrid.dnn.loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"hybrid/dnn/config.json\", \"r\") as fid:                                                                                                                                                                                                                                      \n",
    "    config = json.load(fid)\n",
    "\n",
    "np.random.seed(config[\"seed\"])\n",
    "torch.manual_seed(config[\"seed\"])\n",
    "\n",
    "data_cfg = config[\"data\"]\n",
    "model_cfg = config[\"model\"]\n",
    "opt_cfg = config[\"optimizer\"]\n",
    "out_cfg = config[\"output\"]\n",
    "\n",
    "data_mfccs = np.load(data_cfg[\"mfcc\"], allow_pickle=True)\n",
    "state_seq = np.load(data_cfg[\"state_seq\"], allow_pickle=True)\n",
    "\n",
    "print(\"Preparing data...\\n\")\n",
    "data_ldr = DataLoader(data_cfg)\n",
    "train_features, train_labels, train_labels_onehot, train_utt_to_frames = data_ldr.prepare_data('train')\n",
    "dev_features, dev_labels, dev_labels_onehot, dev_utt_to_frames = data_ldr.prepare_data('dev')\n",
    "test_features, test_labels, test_labels_onehot, test_utt_to_frames = data_ldr.prepare_data('test')\n",
    "\n",
    "feat_dim = (data_ldr.context_size+1)*data_ldr.mfcc_dim\n",
    "n_states = data_ldr.n_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, feat_dim, n_states, hidden_dim, n_layers, dropout):\n",
    "        \"\"\"\n",
    "        Initialized feed forward neural network model.\n",
    "        ---\n",
    "        feat_dim: input feature dimension\n",
    "        n_states: size of the output\n",
    "        hidden_dim: dimension of the hidden layers\n",
    "        n_layers: number of layers\n",
    "        dropout: dropout probabilty for the dropout layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "        self.fc_input = nn.Linear(feat_dim, hidden_dim)\n",
    "        self.fc_output = nn.Linear(hidden_dim, n_states)\n",
    "        self.fc_hidden_list = nn.ModuleList([nn.Linear(hidden_dim, hidden_dim)]*n_layers)\n",
    "        self.nl = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the feedforward network\n",
    "        \"\"\"\n",
    "        x = self.nl(self.fc_input(x))\n",
    "        for i in range(self.n_layers):\n",
    "            x = self.nl(self.fc_hidden_list[i](x))\n",
    "        output = F.leaky_relu(self.fc_output(x))\n",
    "\n",
    "        return output\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    \"\"\"\n",
    "    Training the classifier on frame level labels\n",
    "    \"\"\"\n",
    "    classifier.train()\n",
    "    perm = np.random.permutation(train_features.shape[0])\n",
    "    train_loss, pred_multi, gt_multi = [], [], []\n",
    "    n_iter = 0\n",
    "    start = time.time()\n",
    "    time_per_iter = [0]*4\n",
    "    for i in range(0, len(perm), batch_size):\n",
    "        idx = perm[i:i+batch_size]\n",
    "        train_Xs = torch.tensor(train_features[idx], dtype=torch.float32).to(device)\n",
    "        train_Ys = torch.tensor(train_labels[idx], dtype=torch.long).to(device)\n",
    "        pred_Ys = classifier(train_Xs)\n",
    "        loss = loss_function(pred_Ys, train_Ys)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(classifier.parameters(), 5.0)\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.cpu().item())\n",
    "        pred_multi.append(np.argmax(pred_Ys.cpu().data.numpy(), axis=1))\n",
    "        gt_multi.append(train_Ys.cpu().data.numpy())\n",
    "    pred_multi, gt_multi = np.concatenate(pred_multi, axis=0), np.concatenate(gt_multi, axis=0)\n",
    "    accuracy = 100*len(np.where((pred_multi - gt_multi)==0)[0])/len(pred_multi)\n",
    "    print(\"Epoch: %d, Training loss: %.2f, Accuracy: %.2f, Time elapsed: %.2f seconds\" % (epoch, np.mean(train_loss), accuracy, time.time() - start))\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "def test(features, labels, classifier_test=None):\n",
    "    \"\"\"\n",
    "    Training the classifier on frame level labels\n",
    "    \"\"\"\n",
    "    if classifier_test is None:\n",
    "        classifier_test = torch.load(save_model_fn)\n",
    "    classifier_test.eval()\n",
    "    test_loss, pred_multi, gt_multi = [], [], []\n",
    "    n_iter = 0\n",
    "    start = time.time()\n",
    "    for i in range(0, len(features), test_batch_size):\n",
    "        n_iter += 1\n",
    "        idx = list(range(i, min(i+test_batch_size, len(features))))\n",
    "        test_Xs = torch.tensor(features[idx], dtype=torch.float32).to(device)\n",
    "        test_Ys = torch.tensor(labels[idx], dtype=torch.long).to(device)\n",
    "        pred_Ys = classifier_test(test_Xs)\n",
    "        loss = loss_function(pred_Ys, test_Ys)\n",
    "        test_loss.append(loss.cpu().item())\n",
    "        pred_multi.append(np.argmax(pred_Ys.cpu().data.numpy(), axis=1))\n",
    "        gt_multi.append(test_Ys.cpu().data.numpy())\n",
    "\n",
    "    pred_multi, gt_multi = np.concatenate(pred_multi, axis=0), np.concatenate(gt_multi, axis=0)\n",
    "    accuracy = 100*len(np.where((pred_multi - gt_multi)==0)[0])/len(pred_multi)\n",
    "\n",
    "    print(\"Dev Accuracy: %.2f, Time elapsed: %.2f seconds\" % (accuracy, time.time() - start))\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "def main_train():\n",
    "    print(\"Training begins ...\\n\")\n",
    "    best_accuracy = 0\n",
    "    for epoch in range(tot_epoch):\n",
    "        train_accuracy = train(epoch)\n",
    "        dev_accuracy = test(dev_features, dev_labels, classifier)\n",
    "        if dev_accuracy > best_accuracy:\n",
    "            best_epoch = epoch\n",
    "            torch.save(classifier, save_model_fn)\n",
    "            best_accuracy = dev_accuracy\n",
    "    print('\\nBest dev accuracy: %.2f at epoch: %d' % (best_accuracy, best_epoch))\n",
    "\n",
    "def main_test():\n",
    "    accuracy = test(test_features, test_labels)\n",
    "    print('\\nAccuracy on test set: %.2sf' % (accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training begins ...\n",
      "\n",
      "Epoch: 0, Training loss: 1.97, Accuracy: 44.92, Time elapsed: 743.94 seconds\n",
      "Dev Accuracy: 48.40, Time elapsed: 64.28 seconds\n",
      "Epoch: 1, Training loss: 1.38, Accuracy: 58.82, Time elapsed: 868.08 seconds\n",
      "Dev Accuracy: 51.56, Time elapsed: 44.81 seconds\n",
      "Epoch: 2, Training loss: 1.22, Accuracy: 62.86, Time elapsed: 1299.72 seconds\n",
      "Dev Accuracy: 52.96, Time elapsed: 66.48 seconds\n",
      "Epoch: 3, Training loss: 1.13, Accuracy: 65.78, Time elapsed: 411.45 seconds\n",
      "Dev Accuracy: 53.61, Time elapsed: 7.49 seconds\n",
      "Epoch: 4, Training loss: 1.05, Accuracy: 67.86, Time elapsed: 330.34 seconds\n",
      "Dev Accuracy: 53.15, Time elapsed: 7.12 seconds\n",
      "Epoch: 5, Training loss: 0.99, Accuracy: 69.63, Time elapsed: 334.77 seconds\n",
      "Dev Accuracy: 53.46, Time elapsed: 7.02 seconds\n",
      "Epoch: 6, Training loss: 0.95, Accuracy: 71.02, Time elapsed: 300.05 seconds\n",
      "Dev Accuracy: 52.31, Time elapsed: 7.76 seconds\n",
      "Epoch: 7, Training loss: 0.91, Accuracy: 72.26, Time elapsed: 288.21 seconds\n",
      "Dev Accuracy: 52.51, Time elapsed: 6.54 seconds\n",
      "Epoch: 8, Training loss: 0.88, Accuracy: 73.19, Time elapsed: 299.84 seconds\n",
      "Dev Accuracy: 53.15, Time elapsed: 6.78 seconds\n",
      "Epoch: 9, Training loss: 0.85, Accuracy: 74.01, Time elapsed: 296.61 seconds\n",
      "Dev Accuracy: 53.09, Time elapsed: 6.36 seconds\n",
      "\n",
      "Best dev accuracy: 53.61 at epoch: 3\n"
     ]
    }
   ],
   "source": [
    "tot_epoch = opt_cfg[\"max_epochs\"]\n",
    "hidden_dim = model_cfg[\"hidden_dim\"]\n",
    "n_layers = model_cfg[\"n_layers\"]\n",
    "dropout = model_cfg[\"dropout_probability\"]\n",
    "\n",
    "batch_size = opt_cfg[\"batch_size\"]\n",
    "test_batch_size = opt_cfg[\"test_batch_size\"]\n",
    "\n",
    "save_model_fn = os.path.join(out_cfg[\"save_dir\"], \"dnn_model.pkl\")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if device == 'cuda':\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "classifier = FeedForward(feat_dim, n_states, hidden_dim, n_layers, dropout).to(device)\n",
    "# classifier.apply(init_weights)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = getattr(torch.optim, opt_cfg[\"type\"])(list(classifier.parameters()))\n",
    "\n",
    "main_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512 1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training begins ...\n",
      "\n",
      "GeForce GTX 960\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "Epoch: 0, Training loss: 0.75, Accuracy: 77.59, Time elapsed: 195.10 seconds\n",
      "Dev Accuracy: 52.13, Time elapsed: 3.90 seconds\n",
      "GeForce GTX 960\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-d7e31a9ae6ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[0mhidden_dim_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[0mn_layers_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m \u001b[0mbest_hidden\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbest_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_dim_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_layers_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-52-d7e31a9ae6ea>\u001b[0m in \u001b[0;36mtuning\u001b[1;34m(hidden_dim_list, n_layers_list)\u001b[0m\n\u001b[0;32m    117\u001b[0m             \u001b[0mloss_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt_cfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"type\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m             \u001b[0mtrain_acc_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdev_acc_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_loss_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdev_loss_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbest_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmain_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbest_accuracy\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mtotal_acc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_model_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-52-d7e31a9ae6ea>\u001b[0m in \u001b[0;36mmain_train\u001b[1;34m()\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Allocated:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory_allocated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m1024\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'GB'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Cached:   '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory_cached\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m1024\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'GB'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m         \u001b[0mtrain_accuracy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m         \u001b[0mdev_accuracy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdev_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdev_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdev_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[0mtrain_acc_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_accuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-52-d7e31a9ae6ea>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mtrain_Xs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mtrain_Ys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mpred_Ys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_Xs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_Ys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_Ys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-2da37ae88113>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc_hidden_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\activation.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m    912\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    913\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 914\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    915\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TODO: tune on the dev set\n",
    "# may want to set up function or chunk of code here to perform tuning\n",
    "# call train on training set, call test on dev, save/plot/compare results\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    \"\"\"\n",
    "    Training the classifier on frame level labels\n",
    "    \"\"\"\n",
    "    classifier.train()\n",
    "    perm = np.random.permutation(train_features.shape[0])\n",
    "    train_loss, pred_multi, gt_multi = [], [], []\n",
    "    n_iter = 0\n",
    "    start = time.time()\n",
    "    time_per_iter = [0]*4\n",
    "    for i in range(0, len(perm), batch_size):\n",
    "        idx = perm[i:i+batch_size]\n",
    "        train_Xs = torch.tensor(train_features[idx], dtype=torch.float32).to(device)\n",
    "        train_Ys = torch.tensor(train_labels[idx], dtype=torch.long).to(device)\n",
    "        pred_Ys = classifier(train_Xs)\n",
    "        loss = loss_function(pred_Ys, train_Ys)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(classifier.parameters(), 5.0)\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.cpu().item())\n",
    "        pred_multi.append(np.argmax(pred_Ys.cpu().data.numpy(), axis=1))\n",
    "        gt_multi.append(train_Ys.cpu().data.numpy())\n",
    "    pred_multi, gt_multi = np.concatenate(pred_multi, axis=0), np.concatenate(gt_multi, axis=0)\n",
    "    accuracy = 100*len(np.where((pred_multi - gt_multi)==0)[0])/len(pred_multi)\n",
    "    print(\"Epoch: %d, Training loss: %.2f, Accuracy: %.2f, Time elapsed: %.2f seconds\" % (epoch, np.mean(train_loss), accuracy, time.time() - start))\n",
    "\n",
    "    return accuracy,train_loss\n",
    "\n",
    "def test(features, labels, classifier_test=None):\n",
    "    \"\"\"\n",
    "    Training the classifier on frame level labels\n",
    "    \"\"\"\n",
    "    if classifier_test is None:\n",
    "        classifier_test = torch.load(save_model_fn)\n",
    "    classifier_test.eval()\n",
    "    test_loss, pred_multi, gt_multi = [], [], []\n",
    "    n_iter = 0\n",
    "    start = time.time()\n",
    "    for i in range(0, len(features), test_batch_size):\n",
    "        n_iter += 1\n",
    "        idx = list(range(i, min(i+test_batch_size, len(features))))\n",
    "        test_Xs = torch.tensor(features[idx], dtype=torch.float32).to(device)\n",
    "        test_Ys = torch.tensor(labels[idx], dtype=torch.long).to(device)\n",
    "        pred_Ys = classifier_test(test_Xs)\n",
    "        loss = loss_function(pred_Ys, test_Ys)\n",
    "        test_loss.append(loss.cpu().item())\n",
    "        pred_multi.append(np.argmax(pred_Ys.cpu().data.numpy(), axis=1))\n",
    "        gt_multi.append(test_Ys.cpu().data.numpy())\n",
    "\n",
    "    pred_multi, gt_multi = np.concatenate(pred_multi, axis=0), np.concatenate(gt_multi, axis=0)\n",
    "    accuracy = 100*len(np.where((pred_multi - gt_multi)==0)[0])/len(pred_multi)\n",
    "\n",
    "    print(\"Dev Accuracy: %.2f, Time elapsed: %.2f seconds\" % (accuracy, time.time() - start))\n",
    "\n",
    "    return accuracy,test_loss\n",
    "\n",
    "def main_train():\n",
    "    print(\"Training begins ...\\n\")\n",
    "    best_accuracy = 0\n",
    "    train_acc_list = []\n",
    "    dev_acc_list = []\n",
    "    train_loss_list = []\n",
    "    dev_loss_list = []\n",
    "    for epoch in range(tot_epoch):\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        if device.type == 'cuda':\n",
    "            print(torch.cuda.get_device_name(0))\n",
    "            print('Memory Usage:')\n",
    "            print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "            print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n",
    "        train_accuracy,train_loss = train(epoch)\n",
    "        dev_accuracy,dev_loss = test(dev_features, dev_labels, classifier)\n",
    "        train_acc_list.append(train_accuracy)\n",
    "        dev_acc_list.append(dev_accuracy)\n",
    "        train_loss_list.append(np.sum(train_loss))\n",
    "        dev_loss_list.append(np.sum(dev_loss))\n",
    "        if dev_accuracy > best_accuracy:\n",
    "            best_epoch = epoch\n",
    "            #torch.save(classifier, save_model_fn)\n",
    "            best_accuracy = dev_accuracy\n",
    "    print('\\nBest dev accuracy: %.2f at epoch: %d' % (best_accuracy, best_epoch))\n",
    "    return train_acc_list,dev_acc_list,train_loss_list,dev_loss_list,best_accuracy\n",
    "\n",
    "def main_test():\n",
    "    accuracy = test(test_features, test_labels)\n",
    "    print('\\nAccuracy on test set: %.2sf' % (accuracy))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def tuning(hidden_dim_list,n_layers_list):\n",
    "    tot_epoch = 5\n",
    "    dropout = model_cfg[\"dropout_probability\"]\n",
    "    batch_size = opt_cfg[\"batch_size\"]\n",
    "    test_batch_size = opt_cfg[\"test_batch_size\"]\n",
    "    total_acc = 0\n",
    "    best_hidden =0\n",
    "    best_layer = 0\n",
    "    \n",
    "    for hidden_dim in hidden_dim_list:\n",
    "        for n_layers in n_layers_list:\n",
    "            save_model_fn = os.path.join(out_cfg[\"save_dir\"], \"dnn_model.pkl\")\n",
    "            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "            if device == 'cuda':\n",
    "                torch.backends.cudnn.deterministic = True\n",
    "            torch.cuda.empty_cache()\n",
    "            with torch.no_grad():\n",
    "                classifier = FeedForward(feat_dim, n_states, hidden_dim, n_layers, dropout).to(device)\n",
    "            # classifier.apply(init_weights)\n",
    "            loss_function = nn.CrossEntropyLoss()\n",
    "            optimizer = getattr(torch.optim, opt_cfg[\"type\"])(list(classifier.parameters()))\n",
    "            train_acc_list,dev_acc_list,train_loss_list,dev_loss_list,best_accuracy = main_train()\n",
    "            if best_accuracy > total_acc:\n",
    "                torch.save(classifier, save_model_fn)\n",
    "                total_acc = best_accuracy\n",
    "                best_hidden = hidden_dim\n",
    "                best_layer = n_layers\n",
    "    return best_hidden,best_layer\n",
    "\n",
    "\n",
    "# (hidden_dim = 512 ,model_cfg[\"n_layers\"] = 1)\n",
    "\n",
    "hidden_dim_list = [128,256]\n",
    "n_layers_list = [1,2]\n",
    "best_hidden,best_layer = tuning(hidden_dim_list,n_layers_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_cfg[\"batch_size\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save log-emission probabilities using the best model saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_emission(utt_to_frames_dict, features, prior, temp_parameter, best_model_path):\n",
    "    \"\"\"\n",
    "    Save posteriors using the trained model\n",
    "    \"\"\"\n",
    "    classifier_eval = torch.load(best_model_path)\n",
    "    classifier_eval.eval()\n",
    "    log_emission = []\n",
    "    n_iter = 0\n",
    "    for utt_idx in range(len(utt_to_frames_dict)):\n",
    "        frame_id = utt_to_frames_dict[utt_idx]\n",
    "        log_emission_utt = []\n",
    "        for i in range(0, len(frame_id), batch_size):\n",
    "            idx = frame_id[i:i+batch_size]\n",
    "            Xs = torch.tensor(itemgetter(*idx)(features), dtype=torch.float32).to(device)\n",
    "            log_pred_Ys = F.log_softmax(classifier_eval(Xs), dim=1).cpu().data.numpy()\n",
    "            log_emission_utt.append(log_pred_Ys  - temp_parameter*np.log(prior))\n",
    "        log_emission_utt = np.concatenate(log_emission_utt, axis=0)\n",
    "        log_emission.append(log_emission_utt)\n",
    "\n",
    "    return log_emission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_parameter = out_cfg[\"temp_parameter\"]\n",
    "print(\"Saving log emissions for temperature %.1f ...\\n\" % (temp_parameter))\n",
    "prior = data_ldr.get_prior()\n",
    "train_log_emission =  get_log_emission(train_utt_to_frames, train_features, prior, temp_parameter, save_model_fn)\n",
    "dev_log_emission = get_log_emission(dev_utt_to_frames, dev_features, prior, temp_parameter, save_model_fn)\n",
    "test_log_emission = get_log_emission(test_utt_to_frames, test_features, prior, temp_parameter, save_model_fn)\n",
    "log_emission_dict = {'Ytrain': train_log_emission, 'Ydev': dev_log_emission, 'Ytest': test_log_emission}\n",
    "np.savez_compressed(os.path.join('hybrid/data/log_emission/log_emission_'+str(temp_parameter)+'.npz'), **log_emission_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMM inference using the posterior from neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_multiple_digit = np.load(\"hybrid/data/mfccs/mfccs_multiple.npz\", allow_pickle=True)\n",
    "\n",
    "with open(\"hybrid/hmm/models/multiple_digit_model.pkl\", \"rb\") as f:\n",
    "    full_model_trained = pkl.load(f)\n",
    "\n",
    "log_emission_1 = np.load('hybrid/data/log_emission/log_emission_1.0.npz', allow_pickle=True)\n",
    "log_emission_0 = np.load('hybrid/data/log_emission/log_emission_0.0.npz', allow_pickle=True)\n",
    "    \n",
    "def get_test_wer(model, posterior=None):\n",
    "    test_wer = model.test(data_multiple_digit[\"Xtest\"], data_multiple_digit[\"Ytest\"], posterior)\n",
    "    print(\"{:.2f}% TEST WER\".format(test_wer * 100.))\n",
    "\n",
    "print('Baseline performance of the trained model')\n",
    "get_test_wer(full_model_trained)\n",
    "\n",
    "print('Performance of the trained model with normalized emission probabilities')\n",
    "get_test_wer(full_model_trained, log_emission_1[\"Ytest\"])\n",
    "\n",
    "print('Performance of the trained model with unnormalized emission probabilities')\n",
    "get_test_wer(full_model_trained, log_emission_0[\"Ytest\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
